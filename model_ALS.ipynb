{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f8b05ea",
   "metadata": {},
   "source": [
    "# Project Assignment: Short Video Recommender System (KuaiRec)\n",
    "\n",
    "Dataset Source: [Kuairec](https://kuairec.com/)\n",
    "\n",
    "Arxiv Paper: [KuaiRec: A Fully-observed Dataset and Insights for Evaluating Recommender Systems](https://arxiv.org/pdf/2202.10842)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c028caae",
   "metadata": {},
   "source": [
    "## Dataset import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d87e0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://nas.chongminggao.top:4430/datasets/KuaiRec.zip --no-check-certificate\n",
    "!unzip KuaiRec.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b152809c",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1fe6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "# I get my dataset from a Kaggle input\n",
    "DATA_PATH = \"/kaggle/input/kuairec/KuaiRec 2.0/data\"\n",
    "if not os.path.exists(DATA_PATH):\n",
    "   DATA_PATH = f\"{os.getcwd()}/KuaiRec/data\"\n",
    "if not os.path.exists(DATA_PATH):\n",
    "   DATA_PATH = f\"{os.getcwd()}/KuaiRec 2.0/data\"\n",
    "if not os.path.exists(DATA_PATH):\n",
    "   raise FileNotFoundError(\"KuaiRec dataset not found. Please check the path.\")\n",
    "\n",
    "DATA_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be7809b",
   "metadata": {},
   "source": [
    "# Step 1: Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0db923c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_clear(df : pd.DataFrame) -> pd.DataFrame:\n",
    "    # Date is time in a weird format\n",
    "\n",
    "    # Time and Date are duplicated of timestamp, we can drop them\n",
    "    df.drop(columns=[\"time\", \"date\"], inplace=True)\n",
    "    # Not a problem, we want to keep the data for the density\n",
    "    df = df.astype({\n",
    "        \"user_id\": \"int32\",\n",
    "        \"video_id\": \"int32\",\n",
    "        \"play_duration\":\"int32\",\n",
    "        \"timestamp\": \"int64\",\n",
    "        \"watch_ratio\": \"float32\"}, errors=\"ignore\")\n",
    "    \n",
    "    # Drop duplicates\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    df.dropna(inplace=True)\n",
    "    df = df[df[\"timestamp\"] >= 0]\n",
    "    \n",
    "    df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"], unit=\"s\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54787262",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_describe(df : pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Custom describe for datasets containing user_id and video_id\n",
    "    \"\"\"\n",
    "    print(f\"Shape of the small matrix: {df.shape}\")\n",
    "    unique_users = df[\"user_id\"].nunique()\n",
    "    unique_posts = df[\"video_id\"].nunique()\n",
    "    print(f\"Number of unique users: {unique_users}\")\n",
    "    print(f\"Number of unique posts: {unique_posts}\")\n",
    "    print(f\"Matrix sparsity: {len(df) /(unique_posts * unique_users) * 100}%\")\n",
    "    return df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f60bda",
   "metadata": {},
   "source": [
    "## Small matrix\n",
    "\n",
    "This table has a density of 99.6%. This means that 99.6% of the entries in the matrix are non-zero, indicating that most users have interacted with most items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf0fcd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_matrix = pd.read_csv(f\"{DATA_PATH}/small_matrix.csv\")\n",
    "\n",
    "small_matrix = data_clear(small_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb76158",
   "metadata": {},
   "source": [
    "## Big matrix\n",
    "\n",
    "This table has a density of 16.3%. We will use this matrix for our training and testing.\n",
    "\n",
    "It contains more interactions with the same users/items of the small matrix. We do not need to substract the small matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0561f81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_matrix = pd.read_csv(f\"{DATA_PATH}/big_matrix.csv\")\n",
    "\n",
    "big_matrix = data_clear(big_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115cef26",
   "metadata": {},
   "source": [
    "## Caption Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d34e219",
   "metadata": {},
   "outputs": [],
   "source": [
    "caption_category = pd.read_csv(f\"{DATA_PATH}/kuairec_caption_category.csv\", lineterminator='\\n')\n",
    "caption_category"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203c6994",
   "metadata": {},
   "source": [
    "## Misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d43e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Proportion of small_matrix relative to big_matrix: {small_matrix.shape[0] * 100 / big_matrix.shape[0]:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b262349",
   "metadata": {},
   "source": [
    "# Step 2: Feature Engineering\n",
    "\n",
    "Nothing required here. ALS needs the matrix of user-item interactions. We will use `small_matrix.csv` for training and testing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d13e82d",
   "metadata": {},
   "source": [
    "# Step 3: Alternating Least Squares (ALS) Model\n",
    "\n",
    "Considering that we only have implicit feedback, ALS can work well. We will not use demographic data for this simple model. This algorithm is mostly used for sparse datasets.\n",
    "\n",
    "We will use the ALS algorithm from pyspark.ml.recommendation with hyperparameters tuning and cross-validation.\n",
    "\n",
    "The model is cut into 4 parts:\n",
    "- Data preparation and tuning\n",
    "- Model training\n",
    "- Model evaluation\n",
    "- Model saving\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a7f59c",
   "metadata": {},
   "source": [
    "### Pyspark imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f682a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.ml.recommendation import ALS, ALSModel\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# To evaluate the model with RMSE\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "# For hyperparameter tuning\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder, CrossValidatorModel, Model\n",
    "\n",
    "print(f\"Spark version: {pyspark.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"KuaiRec ALS\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea58e2e",
   "metadata": {},
   "source": [
    "### Data preparation for Pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9284ccfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We load directly from the CSV to avoid memory issues\n",
    "# TODO: Maybe later on use parquet files for cleaned up data\n",
    "small_matrix = spark.read.csv(\n",
    "    f\"{DATA_PATH}/small_matrix.csv\",\n",
    "    header=True,\n",
    "    sep=\",\",\n",
    "    nullValue=\"\",\n",
    "    # We have to infer for correct types\n",
    "    inferSchema=True,\n",
    ").select(\"user_id\", \"video_id\", \"watch_ratio\").na.drop(subset=[\"user_id\", \"video_id\", \"watch_ratio\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e203086",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Makes a lot of problems, we will not use it for now\n",
    "big_matrix = spark.read.csv(\n",
    "    f\"{DATA_PATH}/big_matrix.csv\",\n",
    "    header=True,\n",
    "    sep=\",\",\n",
    "    inferSchema=True,\n",
    "    nullValue=\"\",\n",
    ").select(\"user_id\", \"video_id\", \"watch_ratio\").na.drop(subset=[\"user_id\", \"video_id\", \"watch_ratio\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40a0fb3",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning and Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0fa9cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALS model configuration\n",
    "als = ALS(\n",
    "    maxIter=10,\n",
    "    rank=10,\n",
    "    userCol=\"user_id\",\n",
    "    itemCol=\"video_id\",\n",
    "    ratingCol=\"watch_ratio\",\n",
    "    implicitPrefs=True,\n",
    ")\n",
    "\n",
    "# For CrossValidator\n",
    "params = ParamGridBuilder() \\\n",
    "    .addGrid(als.maxIter, [10, 15]) \\\n",
    "    .addGrid(als.regParam, [0.09, 0.1]) \\\n",
    "    .build()\n",
    "\n",
    "\n",
    "# RMSE\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"watch_ratio\", predictionCol=\"prediction\")\n",
    "\n",
    "\n",
    "# CrossValidator\n",
    "cvs = CrossValidator(\n",
    "    estimator=als,\n",
    "    estimatorParamMaps=params,\n",
    "    evaluator=evaluator,\n",
    "    # Between 2 and 5\n",
    "    numFolds=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00765697",
   "metadata": {},
   "source": [
    "### Training\n",
    "Now with the training, we should have:\n",
    "\n",
    "R ≈ U x V\n",
    "\n",
    "Where:\n",
    "- R is the user-item interaction matrix\n",
    "- U is the user feature matrix\n",
    "- V is the item feature matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795db8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "(training, test) = small_matrix.randomSplit([0.8, 0.2], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f41b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the ALS model on the train data\n",
    "models : CrossValidatorModel = cvs.fit(training)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa2eeea",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc58f762",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the best model from the CrossValidator\n",
    "pyspark_als_model : Model = models.bestModel\n",
    "predictions = pyspark_als_model.transform(test)\n",
    "rmse = evaluator.evaluate(predictions.na.drop())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f070cda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"RMSE: {rmse}\")\n",
    "print(f\"Rank: {pyspark_als_model.rank}\")\n",
    "print(f\"MaxIter: {pyspark_als_model._java_obj.parent().getMaxIter()}\")\n",
    "print(f\"RegParam: {pyspark_als_model._java_obj.parent().getRegParam()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbc33e9",
   "metadata": {},
   "source": [
    "### Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9772f03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyspark_als_model.save(\"pyspark_als_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac67168",
   "metadata": {},
   "source": [
    "# Step 4: ALS Recommendation\n",
    "\n",
    "- Predict which videos are likely to be enjoyed by each user in the test set\n",
    "- Generate a top-N ranked list of recommendations for each user"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85825f1",
   "metadata": {},
   "source": [
    "### Loading model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d517297",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    pyspark_als_model\n",
    "except NameError:\n",
    "    print(\"Model not found. Trying to load it.\")\n",
    "    if os.path.exists(\"pyspark_als_model\"):\n",
    "        print(\"Model found. Loading it.\")\n",
    "        pyspark_als_model = ALSModel.load(\"pyspark_als_model\")\n",
    "        print(\"Model loaded.\")\n",
    "    else:\n",
    "        print(\"Model not found. Please train the model first.\")\n",
    "        raise FileNotFoundError(\"Model not found. Please train the model first.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f057ed1",
   "metadata": {},
   "source": [
    "### Recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a12d13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you have not already used Caption Category\n",
    "caption_category = pd.read_csv(f\"{DATA_PATH}/kuairec_caption_category.csv\", lineterminator='\\n')\n",
    "\n",
    "def video_id_to_caption(video_id: int) -> str:\n",
    "    \"\"\"\n",
    "    Get the caption of a video from its id\n",
    "\n",
    "    Args:\n",
    "        video_id (int): The id of the video\n",
    "\n",
    "    Returns:\n",
    "        str: The caption of the video\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the caption from the video_id\n",
    "    match = caption_category[caption_category[\"video_id\"] == video_id][\"caption\"]\n",
    "    if not match.empty and match.values[0] == match.values[0]:  # check not NaN\n",
    "        return str(match.values[0])\n",
    "    else:\n",
    "        return \"Unknown\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b679d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All user recommendations\n",
    "recommends = pyspark_als_model.recommendForAllUsers(10)\n",
    "recommends_df = recommends.toPandas()\n",
    "\n",
    "# Explode to have each line as a recommendation\n",
    "recommends_df = recommends_df.explode(\"recommendations\")\n",
    "recommends_df[\"recommendations\"] = recommends_df[\"recommendations\"].apply(\n",
    "    lambda x: f\"{video_id_to_caption(x[0])}\")\n",
    "recommends_df.set_index(\"user_id\", inplace=True)\n",
    "recommends_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b780d633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 5 users recommendations\n",
    "top_users = [120, 165, 357, 1314, 2118]\n",
    "top_users_recommends_df = recommends_df[recommends_df.index.isin(top_users)]\n",
    "top_users_recommends_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f550fd",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "- Choose suitable metrics (e.g., Precision@K, Recall@K, MAP, NDCG)\n",
    "- Evaluate performance and provide interpretations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9adbd477",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import collect_list\n",
    "from pyspark.sql.functions import col, expr\n",
    "from sklearn.metrics import ndcg_score\n",
    "\n",
    "ground_truth = test.groupBy(\"user_id\") \\\n",
    "    .agg(collect_list(\"video_id\").alias(\"true_items\"))\n",
    "\n",
    "predicted = recommends.select(\n",
    "    col(\"user_id\"),\n",
    "    expr(\"transform(recommendations, x -> x.video_id)\").alias(\"pred_items\")\n",
    ")\n",
    "ranking_df = predicted.join(ground_truth, on=\"user_id\", how=\"inner\")\n",
    "\n",
    "ranking_pd = ranking_df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d9d7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_at_k(y_true, y_pred, k):\n",
    "    return len(set(y_true) & set(y_pred[:k])) / len(set(y_true)) if y_true else 0\n",
    "\n",
    "def map_at_k(y_true, y_pred, k):\n",
    "    score = 0.0\n",
    "    hit_count = 0.0\n",
    "    for i, p in enumerate(y_pred[:k]):\n",
    "        if p in y_true:\n",
    "            hit_count += 1.0\n",
    "            score += hit_count / (i + 1.0)\n",
    "    return score / min(len(y_true), k) if y_true else 0.0\n",
    "\n",
    "def ndcg_at_k(y_true, y_pred, k):\n",
    "    relevance = [1 if item in y_true else 0 for item in y_pred[:k]]\n",
    "    return ndcg_score([relevance], [relevance])\n",
    "recalls, maps, ndcgs = [], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9629d5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in ranking_pd.itertuples():\n",
    "    true_items = row.true_items\n",
    "    pred_items = row.pred_items\n",
    "    \n",
    "    recalls.append(recall_at_k(true_items, pred_items, 10))\n",
    "    maps.append(map_at_k(true_items, pred_items, 10))\n",
    "    ndcgs.append(ndcg_at_k(true_items, pred_items, 10))\n",
    "\n",
    "print(f\"Recall@10: {np.mean(recalls):.4f}\")\n",
    "print(f\"MAP@10:    {np.mean(maps):.4f}\")\n",
    "print(f\"NDCG@10:   {np.mean(ndcgs):.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "REMA1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
